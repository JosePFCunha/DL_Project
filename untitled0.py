# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://c
    olab.research.google.com/drive/1dHoKjcsHNCRlL20cv6BJalxjyTF2Jemt
"""

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential

import matplotlib.pyplot as plt
import numpy as np

#Importar o dataset escolhido

#O dataset escolhido contém 500 imagens de diferentes espécies de cavalos e 527 diferentes imagens de humanos.

import tensorflow_datasets as tfds
tfds.load("horses_or_humans")

def count_n_images_iterator(this_iterator, name, what='images'):
    """
    Function to count the number of items (images/batches) in the dataset that
    is an iterator. Does not return. Prints a string.

    Input:
        this_iterator: tf.data.Dataset
            Iterator containing the data. TensorFlow Dataset.
        name: str
            Name of the dataset. Should be 'train','test','validation'
        what: str
            Specifies if we are dealing with single images or batches. Should
            be 'images' or 'batches'.

    Output:
        None
    """

    count_items = 0
    for item, label in this_iterator:
        count_items += 1

    print(f'The {name} dataset has {count_items} {what}.')

def plot_acuracy_loss(history):
    """
    Adapted from: https://www.tensorflow.org/tutorials/images/classification
    """
    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']

    loss = history.history['loss']
    val_loss = history.history['val_loss']

    epochs_range = range(epochs)

    plt.figure(figsize=(8, 8))
    plt.subplot(1, 2, 1)
    plt.plot(epochs_range, acc, label='Training Accuracy')
    plt.plot(epochs_range, val_acc, label='Validation Accuracy')
    plt.legend(loc='lower right')
    plt.title('Training and Validation Accuracy')

    plt.subplot(1, 2, 2)
    plt.plot(epochs_range, loss, label='Training Loss')
    plt.plot(epochs_range, val_loss, label='Validation Loss')
    plt.legend(loc='upper right')
    plt.title('Training and Validation Loss')
    plt.show()

"""Dividir o dataset em 3
Em Treino, teste e validação
"""

split0, split1, split2 = tfds.even_splits('train', n=3)

split0, split1, split2 = tfds.even_splits('train', n=3)

ds_train, ds_info = tfds.load(
    'horses_or_humans',
    split=split0,
    shuffle_files=False,
    as_supervised=True,
    with_info=True,
    )

ds_test, __ = tfds.load(
    'horses_or_humans',
    split=split1,
    shuffle_files=False,
    as_supervised=True,
    with_info=True,
    )

ds_val, __ = tfds.load(
    'horses_or_humans',
    split=split2,
    shuffle_files=False,
    as_supervised=True,
    with_info=True,
    )

"""As imagens têm a mesma dimensão 300*300 e 3 canais de cor.

"""

print(ds_info)

fig = tfds.show_examples(ds_train, ds_info, rows=4, cols=4)

count_n_images_iterator(ds_train, 'train'),
count_n_images_iterator(ds_test, 'test'),
count_n_images_iterator(ds_val, 'validation')

sub_ds_train = ds_train.take(343)
sub_ds_test = ds_test.take(68)
sub_ds_val = ds_val.take(150)

def normalize_image(image, label):
    return tf.cast(image, tf.float32) /255.0, label

normalize_ds_train = sub_ds_train.map(normalize_image)
normalize_ds_test = sub_ds_test.map(normalize_image)
normalize_ds_val = sub_ds_val.map(normalize_image)

# Reduzimos o tamanho das imagens (apesar de não ser necessário porque o dataset é pequeno) porque deminui o tempo de computação e Regularização implícita: 
Reduzir o tamanho das imagens pode ter um efeito de regularização implícita no modelo, ajudando a evitar overfitting.
Imagens menores tendem a capturar menos detalhes e podem forçar o modelo a aprender representações mais robustas e generalizadas.

def resize_image(image, label, target_height=100, target_width=100):
    return tf.image.resize_with_pad(image, target_height, target_width), label

resized_ds_train = normalize_ds_train.map(resize_image)
resized_ds_test = normalize_ds_test.map(resize_image)
resized_ds_val = normalize_ds_val.map(resize_image)

count_n_images_iterator(resized_ds_train, 'train'),
count_n_images_iterator(resized_ds_test, 'test'),
count_n_images_iterator(resized_ds_val, 'validation')

count_n_images_iterator(resized_ds_train, 'train', 'batches'), # nao sao imagens, sao batches (grupos) de imagens
count_n_images_iterator(resized_ds_test, 'test', 'batches'), # nao sao imagens, sao batches (grupos) de imagens
count_n_images_iterator(resized_ds_val, 'validation', 'batches') # nao sao imagens, sao batches (grupos) de imagens

fig = tfds.show_examples(resized_ds_train, ds_info, rows=4, cols=4)

resized_ds_train = resized_ds_train.batch(128) # depende da memoria disponiveis na maquina, numero maior representa mais dados em memoria
resized_ds_test = resized_ds_test.batch(128) # depende da memoria disponiveis na maquina, numero maior representa mais dados em memoria
resized_ds_val = resized_ds_val.batch(128) # depende da memoria disponiveis na maquina, numero maior representa mais dados em memoria

count_n_images_iterator(resized_ds_train, 'train', 'batches'), # nao sao imagens, sao batches (grupos) de imagens
count_n_images_iterator(resized_ds_test, 'test', 'batches'), # nao sao imagens, sao batches (grupos) de imagens
count_n_images_iterator(resized_ds_val, 'validation', 'batches') # nao sao imagens, sao batches (grupos) de imagens

baseline_model = Sequential([
  layers.Input(shape=(100, 100, 3)),
  layers.Flatten(),
  layers.Dense(300, activation='relu'),
  layers.Dense(100, activation='relu'),
  layers.Dense(10, activation='relu'),
  layers.Dense(1, activation='linear')
])

baseline_model.compile(
    optimizer='adam',
    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
    metrics=['accuracy'])

baseline_model.summary()


epochs=30
history = baseline_model.fit(
  resized_ds_train,
  validation_data=resized_ds_val,
  epochs=epochs
)

epochs=30
history = baseline_model.fit(
  resized_ds_train,
  validation_data=resized_ds_val,
  epochs=epochs
)



plot_acuracy_loss(history)

cnn_model = Sequential([
  layers.Input(shape=(100, 100, 3)),
  layers.Conv2D(16, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Conv2D(32, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Flatten(),
  layers.Dense(128, activation='relu'),
  layers.Dense(64, activation='relu'),
  layers.Dense(32, activation='relu'),
  layers.Dense(1, activation='linear')
])

#Optimizer Adam é utilizado para ter uma learning curve maior no inicio e depois diminui para evitar o efeito "ping-pong"
Desta forma conseguimos acelerar o processo de optimização

cnn_model.compile(
    optimizer='adam',
    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
    metrics=['accuracy'])

cnn_model.summary()

epochs=20
history = cnn_model.fit(
  resized_ds_train,
  validation_data=resized_ds_val,
  epochs=epochs
)

plot_acuracy_loss(history)

# O modelo tem alta Accuracy e pouca loss, para podermos utilizar modelo de regularização vamos piorar os resuultados do modelo.

# Adicionar camadas adicionais ao modelo
baseline_model = Sequential([
  layers.Input(shape=(100, 100, 3)),
  layers.Flatten(),
  layers.Dense(300, activation='relu'),
  layers.Dense(200, activation='relu'),  # Adicione uma camada densa com mais neurônios
  layers.Dense(100, activation='relu'),  # Adicione uma camada densa com mais neurônios
  layers.Dense(10, activation='relu'),
  layers.Dense(1, activation='linear')
])

# Compilar o modelo
baseline_model.compile(
    optimizer='adam',
    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
    metrics=['accuracy'])

# Treinar o modelo por mais épocas

#O objetivo de treinar com mais épocas (epochs) é permitir que o modelo aprenda com mais exemplos de treino e refine seus parâmetros (pesos) ao longo do tempo.
Cada época representa uma passagem completa pelo conjunto de dados de treino durante o processo de treino da Neural Network.
Desta forma aumentando também o overfitting.

epochs=50  # Aumente o número de épocas de treinamento
history = baseline_model.fit(
  resized_ds_train,
  validation_data=resized_ds_val,
  epochs=epochs
)

from tensorflow.keras import regularizers

# Adicionar regularização Dropout

baseline_model = Sequential([
  layers.Input(shape=(100, 100, 3)),
  layers.Flatten(),
  layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001)),  # Adicionando regularização L2
  layers.Dropout(0.5),  # Adicionando Dropout com uma taxa de 0.5
  layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001)),  # Adicionando regularização L2
  layers.Dropout(0.5),  # Adicionando Dropout com uma taxa de 0.5
  layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)),  # Adicionando regularização L2
  layers.Dropout(0.5),  # Adicionando Dropout com uma taxa de 0.5
  layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)),  # Adicionando regularização L2
  layers.Dropout(0.5),  # Adicionando Dropout com uma taxa de 0.5
  layers.Dense(1, activation='linear')
])

# Compilando o modelo
baseline_model.compile(
    optimizer='adam',
    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
    metrics=['accuracy'])

# Treinando o modelo com o número de épocas otimizado anteriormente
epochs = 50
history = baseline_model.fit(
  resized_ds_train,
  validation_data=resized_ds_val,
  epochs=epochs
)

