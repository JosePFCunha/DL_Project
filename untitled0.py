# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dHoKjcsHNCRlL20cv6BJalxjyTF2Jemt
"""

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential

import matplotlib.pyplot as plt
import numpy as np

#importar o dataset escolhigo

import tensorflow_datasets as tfds
tfds.load("horses_or_humans")

def count_n_images_iterator(this_iterator, name, what='images'):
    """
    Function to count the number of items (images/batches) in the dataset that
    is an iterator. Does not return. Prints a string.

    Input:
        this_iterator: tf.data.Dataset
            Iterator containing the data. TensorFlow Dataset.
        name: str
            Name of the dataset. Should be 'train','test','validation'
        what: str
            Specifies if we are dealing with single images or batches. Should
            be 'images' or 'batches'.

    Output:
        None
    """

    count_items = 0
    for item, label in this_iterator:
        count_items += 1

    print(f'The {name} dataset has {count_items} {what}.')

def plot_acuracy_loss(history):
    """
    Adapted from: https://www.tensorflow.org/tutorials/images/classification
    """
    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']

    loss = history.history['loss']
    val_loss = history.history['val_loss']

    epochs_range = range(epochs)

    plt.figure(figsize=(8, 8))
    plt.subplot(1, 2, 1)
    plt.plot(epochs_range, acc, label='Training Accuracy')
    plt.plot(epochs_range, val_acc, label='Validation Accuracy')
    plt.legend(loc='lower right')
    plt.title('Training and Validation Accuracy')

    plt.subplot(1, 2, 2)
    plt.plot(epochs_range, loss, label='Training Loss')
    plt.plot(epochs_range, val_loss, label='Validation Loss')
    plt.legend(loc='upper right')
    plt.title('Training and Validation Loss')
    plt.show()

"""Dividir o dataset em 3
Em Treino, teste e validação
"""

split0, split1, split2 = tfds.even_splits('train', n=3)

split0, split1, split2 = tfds.even_splits('train', n=3)

ds_train, ds_info = tfds.load(
    'horses_or_humans',
    split=split0,
    shuffle_files=False,
    as_supervised=True,
    with_info=True,
    )

ds_test, __ = tfds.load(
    'horses_or_humans',
    split=split1,
    shuffle_files=False,
    as_supervised=True,
    with_info=True,
    )

ds_val, __ = tfds.load(
    'horses_or_humans',
    split=split2,
    shuffle_files=False,
    as_supervised=True,
    with_info=True,
    )

"""As imagens têm a mesma dimensão 300*300 e 3 canais de cor.

"""

print(ds_info)

fig = tfds.show_examples(ds_train, ds_info, rows=4, cols=4)

count_n_images_iterator(ds_train, 'train'),
count_n_images_iterator(ds_test, 'test'),
count_n_images_iterator(ds_val, 'validation')

sub_ds_train = ds_train.take(343)
sub_ds_test = ds_test.take(68)
sub_ds_val = ds_val.take(150)

def normalize_image(image, label):
    return tf.cast(image, tf.float32) /255.0, label

normalize_ds_train = sub_ds_train.map(normalize_image)
normalize_ds_test = sub_ds_test.map(normalize_image)
normalize_ds_val = sub_ds_val.map(normalize_image)

def resize_image(image, label, target_height=100, target_width=100):
    return tf.image.resize_with_pad(image, target_height, target_width), label

resized_ds_train = normalize_ds_train.map(resize_image)
resized_ds_test = normalize_ds_test.map(resize_image)
resized_ds_val = normalize_ds_val.map(resize_image)

count_n_images_iterator(resized_ds_train, 'train'),
count_n_images_iterator(resized_ds_test, 'test'),
count_n_images_iterator(resized_ds_val, 'validation')

count_n_images_iterator(resized_ds_train, 'train', 'batches'), # nao sao imagens, sao batches (grupos) de imagens
count_n_images_iterator(resized_ds_test, 'test', 'batches'), # nao sao imagens, sao batches (grupos) de imagens
count_n_images_iterator(resized_ds_val, 'validation', 'batches') # nao sao imagens, sao batches (grupos) de imagens

fig = tfds.show_examples(resized_ds_train, ds_info, rows=4, cols=4)

resized_ds_train = resized_ds_train.batch(128) # depende da memoria disponiveis na maquina, numero maior representa mais dados em memoria
resized_ds_test = resized_ds_test.batch(128) # depende da memoria disponiveis na maquina, numero maior representa mais dados em memoria
resized_ds_val = resized_ds_val.batch(128) # depende da memoria disponiveis na maquina, numero maior representa mais dados em memoria

count_n_images_iterator(resized_ds_train, 'train', 'batches'), # nao sao imagens, sao batches (grupos) de imagens
count_n_images_iterator(resized_ds_test, 'test', 'batches'), # nao sao imagens, sao batches (grupos) de imagens
count_n_images_iterator(resized_ds_val, 'validation', 'batches') # nao sao imagens, sao batches (grupos) de imagens

baseline_model = Sequential([
  layers.Input(shape=(100, 100, 3)),
  layers.Flatten(),
  layers.Dense(300, activation='relu'),
  layers.Dense(100, activation='relu'),
  layers.Dense(10, activation='relu'),
  layers.Dense(1, activation='linear')
])

baseline_model.compile(
    optimizer='adam',
    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
    metrics=['accuracy'])

baseline_model.summary()

epochs=30
history = baseline_model.fit(
  resized_ds_train,
  validation_data=resized_ds_val,
  epochs=epochs
)

epochs=30
history = baseline_model.fit(
  resized_ds_train,
  validation_data=resized_ds_val,
  epochs=epochs
)

plot_acuracy_loss(history)

cnn_model = Sequential([
  layers.Input(shape=(100, 100, 3)),
  layers.Conv2D(16, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Conv2D(32, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Flatten(),
  layers.Dense(128, activation='relu'),
  layers.Dense(64, activation='relu'),
  layers.Dense(32, activation='relu'),
  layers.Dense(1, activation='linear')
])

cnn_model.compile(
    optimizer='adam',
    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
    metrics=['accuracy'])

cnn_model.summary()

epochs=20
history = cnn_model.fit(
  resized_ds_train,
  validation_data=resized_ds_val,
  epochs=epochs
)

plot_acuracy_loss(history)